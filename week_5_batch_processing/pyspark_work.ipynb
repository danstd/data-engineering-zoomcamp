{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee78f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c5703",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c14da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.write.parquet('zones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043eb00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fhvhv_tripdata_2021-02.parquet\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"fhv\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "#fhv = spark.read.parquet(\"fhvhv_tripdata_2021-02.parquet\")\n",
    "\n",
    "fhv = spark.read.parquet(\"fhv_r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0bd050",
   "metadata": {},
   "source": [
    "### Old Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff345dca",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6478c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-coalesce/\n",
    "\n",
    "# DataFrame repartition\n",
    "fhv_r = fhv.repartition(24)\n",
    "print(fhv_r.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e91ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv_r.write.parquet(\"fhv_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4f1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd() + \"\\\\fhv_r\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a318d",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5def8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Modified from https://www.geeksforgeeks.org/how-to-get-size-of-folder-using-python/\n",
    "\n",
    "output_path = Path(os.getcwd()) / \"fhv_r\"\n",
    "\n",
    "size = 0\n",
    "\n",
    "for path, dirs, files in os.walk(str(output_path)):\n",
    "    for file in files:\n",
    "        size += os.path.getsize(output_path / file)\n",
    "\n",
    "print(f\"Folder size: {round(size / 1048576, 1)} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125fcadb",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9e5747",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv.select(func.count(fhv.pickup_datetime)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe69bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/31407461/datetime-range-filter-in-pyspark-sql\n",
    "\n",
    "fhv_time = fhv.select(func.to_date(fhv.pickup_datetime).alias(\"time\"))\n",
    "\n",
    "date_from = \"2021-02-15 00:00:00\"\n",
    "date_to = \"2021-02-16 00:00:00\"\n",
    "sf = fhv_time.filter(fhv_time.time >= date_from).filter(fhv_time.time < date_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd565d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv.registerTempTable(\"fhv_temp\")\n",
    "\n",
    "trips_15th = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(pickup_datetime) AS trips\n",
    "FROM\n",
    "    fhv_temp\n",
    "WHERE\n",
    "    pickup_datetime >= '2021-02-15 00:00:00'\n",
    "    AND pickup_datetime < '2021-02-16 00:00:00'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd68036",
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.select(func.count(sf.time)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe59efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_15th.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87493a4a",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c369fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_duration = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    MAX(FLOAT(dropoff_datetime) - FLOAT(pickup_datetime)) AS duration\n",
    "FROM\n",
    "    fhv_temp\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9573c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_duration = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_datetime,\n",
    "    (FLOAT(dropoff_datetime) - FLOAT(pickup_datetime)) AS duration\n",
    "FROM\n",
    "    fhv_temp\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b7cdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_duration.join(max_duration, on=\"duration\", how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17c5e31",
   "metadata": {},
   "source": [
    "### 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c294dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dispatch_base = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    dispatching_base_num,\n",
    "    count(dispatching_base_num) as base_count\n",
    "FROM\n",
    "    fhv_temp\n",
    "GROUP BY dispatching_base_num\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "top_dispatch_base.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8a6762",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_dispatch_base.explain()\n",
    "\n",
    "# http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc52aa6",
   "metadata": {},
   "source": [
    "### 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d804fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954cef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4061d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones.registerTempTable(\"zone_temp\")\n",
    "\n",
    "zone_pu = spark.sql(\"\"\"SELECT LocationID AS PULocationID, Zone AS PUZone FROM zone_temp\"\"\")\n",
    "zone_do = spark.sql(\"\"\"SELECT LocationID AS DOLocationID, Zone AS DOZone FROM zone_temp\"\"\")\n",
    "\n",
    "zone_merge = fhv.join(zone_pu, on=\"PULocationID\", how='left')\n",
    "\n",
    "zone_merge = zone_merge.join(zone_do, on=\"DOLocationID\", how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25472968",
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_merge.registerTempTable(\"zone_merge_temp\")\n",
    "\n",
    "top_pairs = spark.sql(\"\"\"\n",
    "SELECT zone_pairs, COUNT(zone_pairs)\n",
    "FROM (\n",
    "SELECT CONCAT(COALESCE(PUZone, 'unknown'), \" / \", COALESCE(DOZone, 'unknown')) as zone_pairs\n",
    "FROM\n",
    "    zone_merge_temp\n",
    "    )\n",
    "GROUP BY zone_pairs\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3192639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pairs.show(n=1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909e922",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "\n",
    "SELECT COUNT(PUZone)\n",
    "FROM\n",
    "    zone_merge_temp\n",
    "WHERE PUZone = 'East New York' AND DOZone = 'East New York'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456fb6f0",
   "metadata": {},
   "source": [
    "### New Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c378f",
   "metadata": {},
   "source": [
    "### 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c559da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"fhvhv_tripdata_2021-06.csv\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"fhv\") \\\n",
    "    .config(\"spark.driver.memory\", \"15g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32cad8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd2c52e",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c44ace19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispatching_base_num',\n",
       " 'pickup_datetime',\n",
       " 'dropoff_datetime',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'SR_Flag',\n",
       " 'Affiliated_base_number']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "file = \"fhvhv_tripdata_2021-06.csv\"\n",
    "with open(file) as r:\n",
    "    headers= pd.read_csv(r, nrows=0).columns.tolist()\n",
    "    \n",
    "headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5d754d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    # types.StructField('hvfhs_license_num', types.StringType(), True),\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropoff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "#fhv = spark.read.parquet(file)\n",
    "\n",
    "fhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01cae1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-coalesce/\n",
    "\n",
    "# DataFrame repartition\n",
    "fhv_r = fhv.repartition(12)\n",
    "print(fhv_r.rdd.getNumPartitions())\n",
    "\n",
    "fhv_r.write.parquet(\"fhv_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "511faeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00001-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00002-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00003-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00004-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00005-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00006-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00007-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00008-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00009-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00010-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "part-00011-548a3904-6f48-4256-8bb0-28745127c741-c000.snappy.parquet\n",
      "Folder size: 260.6 MB\n",
      "Files: 12\n",
      "Average size: 21.717481056849163\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "# Modified from https://www.geeksforgeeks.org/how-to-get-size-of-folder-using-python/\n",
    "\n",
    "output_path = Path(os.getcwd()) / \"fhv_r\"\n",
    "\n",
    "size = 0\n",
    "file_count = 0\n",
    "for path, dirs, files in os.walk(str(output_path)):\n",
    "    for file in files:\n",
    "        file_split = file.split(\".\")\n",
    "        if (file_split[len(file_split) - 1]).lower() == \"parquet\":\n",
    "            print(file)\n",
    "            size += os.path.getsize(output_path / file)\n",
    "            file_count += 1\n",
    "\n",
    "print(f\"Folder size: {round(size / 1048576, 1)} MB\")\n",
    "print(f\"Files: {file_count}\")\n",
    "\n",
    "print(f\"Average size: {(size / 1048576) / file_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23fed9",
   "metadata": {},
   "source": [
    "### 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09265580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| trips|\n",
      "+------+\n",
      "|452470|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fhv_r.registerTempTable(\"fhv_temp\")\n",
    "\n",
    "trips_15th = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(pickup_datetime) AS trips\n",
    "FROM\n",
    "    fhv_temp\n",
    "WHERE\n",
    "    pickup_datetime >= '2021-06-15 00:00:00'\n",
    "    AND pickup_datetime < '2021-06-16 00:00:00'\n",
    "\"\"\")\n",
    "\n",
    "trips_15th.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9675c43",
   "metadata": {},
   "source": [
    "### 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cef14e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|duration|\n",
      "+--------+\n",
      "|   66.88|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_duration = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    MAX((FLOAT(dropoff_datetime) - FLOAT(pickup_datetime)) / 3600) AS duration\n",
    "FROM\n",
    "    fhv_temp\n",
    "WHERE dropoff_datetime IS NOT NULL and pickup_datetime IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "date_duration = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_datetime,\n",
    "    (FLOAT(dropoff_datetime) - FLOAT(pickup_datetime))/ 3600 AS duration\n",
    "FROM\n",
    "    fhv_temp\n",
    "\"\"\")\n",
    "\n",
    "max_duration.show()\n",
    "\n",
    "#date_duration.join(max_duration, on=\"duration\", how=\"inner\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67116e51",
   "metadata": {},
   "source": [
    "### 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e3d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://localhost:4040/jobs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b391fe",
   "metadata": {},
   "source": [
    "### 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "777867f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " PUZone        | Crown Heights North \n",
      " count(PUZone) | 231279              \n",
      "\n"
     ]
    }
   ],
   "source": [
    "zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')\n",
    "\n",
    "zones.registerTempTable(\"zone_temp\")\n",
    "\n",
    "zone_pu = spark.sql(\"\"\"SELECT LocationID AS PULocationID, Zone AS PUZone FROM zone_temp\"\"\")\n",
    "\n",
    "zone_merge = fhv.join(zone_pu, on=\"PULocationID\", how='inner')\n",
    "\n",
    "\n",
    "zone_merge.registerTempTable(\"zone_merge_temp\")\n",
    "\n",
    "top_pu = spark.sql(\"\"\"\n",
    "SELECT PUZone, COUNT(PUZone)\n",
    "FROM\n",
    "    zone_merge_temp\n",
    "where PUZone is not null and PUZone != 'NA'\n",
    "GROUP BY PUZone\n",
    "ORDER BY 2 DESC\n",
    "LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "top_pu.show(n=1, truncate=False, vertical=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
